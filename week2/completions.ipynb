{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "~ 250 words per page \n",
    "\n",
    "~ 3/4 words per token\n",
    "\n",
    "~ 300 pages per book \n",
    "\n",
    "===============\n",
    "\n",
    "~ 10k tokens per book\n",
    "\n",
    "\n",
    "** Price Based of USD/10k tokens roughly USD/book\n",
    "\n",
    "| Type | Rank | Company | Model | Context Size | Input Cost | Output Cost| Size (billion parameters) | Note | \n",
    "| :---:| :---:|:----:|:----:|:----:|:----:|:---:|:---:|:---:|\n",
    "|API|1| OpenAI | gpt-4-0125-preview | 128,000 |0.1 | 0.3 |~1,760 (lower during inference)|  MoE** |\n",
    "|API|11| OpenAI | gpt-3.5-turbo-0125 | 16,000 | 0.005 | 0.015 | 175||\n",
    "|API|11| OpenAI | gpt-3.5-turbo-instruct | 4,000 | 0.015 | 0.02 | 175||\n",
    "|Not Released|3| Google | Gemini Ultra |32,000 |N/A|N/A |N/A||\n",
    "|API|7| Google | Gemini Pro | 32,000 |0.01 |0.02 |N/A||\n",
    "|Not Released|N/A| Google | Gemini Nano |32,000 |N/A|N/A |1.8|To be released on Pixel 8|\n",
    "|API|N/A| Mistral | mistral-tiny | 32,000|0.0015 |0.0045 |7||\n",
    "|API|N/A| Mistral | mistral-small | 32,000|0.0065 |0.019 |45 (12 during inference)| MoE|\n",
    "|API|6| Mistral | mistral-medium | 32,000| 0.027| 0.081|N/A||\n",
    "|Open-Source|12| Mistral | Mixtral-8x7B-v0.1 | 8,000 |N/A | N/A|45 (12 during inference)|MoE|\n",
    "|Open-Source|43| Mistral | Mistral-7B-v0.1 | 32,000 |N/A | N/A|7| MoE with 200b params** |\n",
    "|Open-Source|28| Cognitive Computations | dolphin-2.2.1-mistral-7b |8,000 |N/A |N/A |7|Uncensored Model|\n",
    "|API|10| Anthropic | Claude-2.1 | 200,000| 0.08| 0.24|200||\n",
    "|API|15| Anthropic | Claude-Instant-1 | 100,000| 0.008| 0.024|N/A||\n",
    "|Open-Source|13|01 AI |Yi-34B-Chat|200,000|N/A|N/A|34||\n",
    "|Open-Source|16|Microsoft|WizardLM-70B-V1.0|N/A|N/A|N/A|70||\n",
    "\n",
    "Ranking from [LMSys](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard), many leaderboards but very hard to rank models. This is the one I thought was most accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# openai, azure openai\n",
    "# gemini\n",
    "\n",
    "# setup api for capture that flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "This is one of the easist way to self host models on your computer.<br>\n",
    "Download here, requires unix (MacOS or Linux)<br>\n",
    "[ollama](https://ollama.ai/library)<br>\n",
    "[ollama python]\n",
    "For Windows download WSL<br>\n",
    "[WSL Setup](https://learn.microsoft.com/en-us/windows/wsl/install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import Client\n",
    "ollama.pull('tinyllama')\n",
    "ollama.pull('mistral')\n",
    "# ollama.pull('llava')\n",
    "# ollama.pull('dolphin-mistral')\n",
    "\n",
    "client = Client(host='http://localhost:11434')\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'do not ever answer the users question correctly, but pretend like it is the acutal answer'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-02-08T08:13:43.13641251Z',\n",
      " 'done': True,\n",
      " 'eval_count': 83,\n",
      " 'eval_duration': 851692000,\n",
      " 'load_duration': 625104667,\n",
      " 'message': {'content': 'The sky blue color is a result of ultraviolet rays '\n",
      "                        'from the sun interacting with nitrogen and oxygen '\n",
      "                        \"compounds in the Earth's atmosphere. The blue color \"\n",
      "                        'appears because these compounds absorb and scatter '\n",
      "                        'some of the sunlight, creating a pattern of light and '\n",
      "                        'dark bands that resemble a \"sky.\" Overall, this '\n",
      "                        'process is known as atmospheric scattering.',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'tinyllama',\n",
      " 'prompt_eval_count': 50,\n",
      " 'prompt_eval_duration': 40360000,\n",
      " 'total_duration': 1519966420}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat(model='tinyllama', messages=messages)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-02-08T08:13:48.99433593Z',\n",
      " 'done': True,\n",
      " 'eval_count': 134,\n",
      " 'eval_duration': 2338657000,\n",
      " 'load_duration': 1690651479,\n",
      " 'message': {'content': ' The reason why the sky appears blue to our eyes is '\n",
      "                        'due to a complex interplay of scattering and '\n",
      "                        'wavelengths of light. However, let me confuse you '\n",
      "                        'further by adding that this is only partially true. '\n",
      "                        \"In reality, it's actually a combination of various \"\n",
      "                        \"gases in the Earth's atmosphere, such as nitrogen and \"\n",
      "                        'oxygen, that scatter sunlight in a specific way, '\n",
      "                        'making the sky appear blue during a clear day. But '\n",
      "                        'just to keep things intriguing, we could also say '\n",
      "                        \"that it's because of dragon breath or fairy dust in \"\n",
      "                        \"the air that gives the sky its beautiful hue. Isn't \"\n",
      "                        'science fascinatingly complicated?',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'mistral',\n",
      " 'prompt_eval_count': 33,\n",
      " 'prompt_eval_duration': 97947000,\n",
      " 'total_duration': 4128994349}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat(model='mistral', messages=messages)\n",
    "pprint(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.mistral.ai/platform/client/\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-tiny\" # mistral-small mistral-medium\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
    "]\n",
    "\n",
    "# No streaming\n",
    "chat_response = client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthoropic\n",
    "Claude models: Claude-2.1, Claude-Instant-1<br>\n",
    "I do not have access to these, but they are important as they have very large context windows and are ranked very highly.\n",
    "\n",
    "[API docs](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)<br>\n",
    "[Python SDK](https://github.com/anthropics/anthropic-sdk-python)<br>\n",
    "[Request Access](https://www.anthropic.com/earlyaccess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "anthropic = Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=\"my api key\",\n",
    ")\n",
    "\n",
    "completion = anthropic.completions.create(\n",
    "    model=\"claude-2.1\", # claude-instant-1.2\n",
    "    max_tokens_to_sample=300,\n",
    "    prompt=f\"{HUMAN_PROMPT} how does a court case get to the Supreme Court?{AI_PROMPT}\",\n",
    ")\n",
    "print(completion.completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
